{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import os\n",
    "import json\n",
    "from time import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse as spsp\n",
    "#from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.cuda.dnn import dnn_conv\n",
    "\n",
    "# This sets which GPU to use\n",
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use(\"gpu2\")\n",
    "\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "theano.config.scan.allow_gc = True\n",
    "theano.config.exception_verbosity = 'low'\n",
    "theano.config.warn_float64 = 'raise'\n",
    "theano.config.floatX = 'float32'\n",
    "\n",
    "# Set this to the current working directory\n",
    "# (the directory where the gae.ipynb file is stored)\n",
    "cwd_path = '/home/jsvoboda/Documents/fingerprits/gae/'\n",
    "# Set this to the folder where your data (synthetic fingerprints) are stored\n",
    "data_path = '/media/nas/fingerprints/'\n",
    "\n",
    "logger = logging.getLogger('fingerprints')\n",
    "hdlr = logging.FileHandler(os.path.join(cwd_path, 'training.log'))\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr) \n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from lib import activations\n",
    "from lib import updates\n",
    "from lib import inits\n",
    "from lib.rng import py_rng, np_rng\n",
    "from lib.ops import batchnorm, conv_cond_concat, deconv, dropout, l2normalize\n",
    "from lib.metrics import nnc_score, nnd_score\n",
    "from lib.theano_utils import floatX, sharedX\n",
    "from lib.data_utils import OneHot, shuffle, iter_data, center_crop, patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "N.B. The code from the following imports is lifted from the original [dcgan project](https://github.com/Newmu/dcgan_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_matlab_file(path_file, name_field):\n",
    "    \"\"\"\n",
    "    load '.mat' files\n",
    "    inputs:\n",
    "        path_file, string containing the file pathd\n",
    "        name_field, string containig the field name (default='shape')\n",
    "    warning:\n",
    "        '.mat' files should be saved in the '-v7.3' format\n",
    "    \"\"\"\n",
    "    retry = 1\n",
    "    while retry:\n",
    "        try:\n",
    "            db = h5py.File(path_file, 'r')\n",
    "            ds = db[name_field]\n",
    "            retry = 0\n",
    "        except IOError:\n",
    "            retry = 1\n",
    "        \n",
    "    try:\n",
    "        if 'ir' in ds.keys():\n",
    "            data = np.asarray(ds['data'])\n",
    "            ir   = np.asarray(ds['ir'])\n",
    "            jc   = np.asarray(ds['jc'])\n",
    "            out  = sp.csc_matrix((data, ir, jc)).astype(np.float32)\n",
    "    except AttributeError:\n",
    "        # Transpose in case is a dense matrix because of the row- vs column- major ordering between python and matlab\n",
    "        out = np.asarray(ds).astype(np.float32).T\n",
    "\n",
    "    db.close()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train / test splitting\n",
    "path        = cwd_path\n",
    "fSamples    = open(path + 'dataset.txt', 'w')\n",
    "fTargets    = open(path + 'targets.txt', 'w')\n",
    "fMinutiae   = open(path + 'minutiae.txt', 'w')\n",
    "#dirs        = range(1, 9)\n",
    "samples     = range(1, 1000)\n",
    "#for di in dirs:\n",
    "for sam in samples:\n",
    "    fSamples.write('sample_train_bg_8/%d.png\\n' % (sam))\n",
    "    fTargets.write('sample_train_bg_8_targets_bin/%d.png\\n' % (sam))\n",
    "    fMinutiae.write('sample_train_bg_8_targets_minutiae/feats_%d.mat\\n' % (sam))\n",
    "samples     = range(1, 1000)\n",
    "for sam in samples:\n",
    "    fSamples.write('sample_test_bg_2/%d.png\\n' % (sam))\n",
    "    fTargets.write('sample_test_bg_2_targets_bin/%d.png\\n' % (sam))\n",
    "    fMinutiae.write('sample_test_bg_2_targets_minutiae/feats_%d.mat\\n' % (sam))\n",
    "fSamples.close()\n",
    "fTargets.close()\n",
    "fMinutiae.close()\n",
    "\n",
    "fEval = open(path + 'eval.txt', 'w')\n",
    "samples = range(1, 38)\n",
    "eval_dir = 'eval_data/'\n",
    "if os.path.exists(eval_dir):\n",
    "    files = os.listdir(eval_dir)\n",
    "    for aFile in files:\n",
    "        if os.path.splitext(aFile)[1] in ['.png', '.jpg', '.bmp']:\n",
    "            fEval.write(os.path.join(eval_dir, aFile) + '\\n')\n",
    "fEval.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from skimage import filters\n",
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "import skimage.filters as skif\n",
    "import skimage.morphology as skim\n",
    "import scipy.ndimage.filters as spif\n",
    "import skimage.util as skiu\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return (0.299 * rgb[:, :, 0] + 0.587 * rgb[:, :, 1] + 0.114 * rgb[:, :, 2]).astype(np.uint8)\n",
    "\n",
    "def flipInPlace(image):\n",
    "    origSizeX, origSizeY = image.shape[0], image.shape[1]\n",
    "\n",
    "    flippedImages = {\n",
    "        0: image, # Original image\n",
    "        1: np.flipud(image), # Horizontal flip\n",
    "        2: np.fliplr(image), # Vertical flip\n",
    "        3: np.flipud(np.fliplr(image)) # Hor & Vert flip\n",
    "    }\n",
    "\n",
    "    flipType = np.random.randint(4)\n",
    "\n",
    "    return flippedImages[flipType]\n",
    "\n",
    "# dataset\n",
    "class FaceDataset(object):\n",
    "    def __init__(self, dataset_txt, targets_txt, minutiae_txt, face_paths, eval_txt, eval_paths, batch_size = 12, epoch_size = 1):\n",
    "        self.dataset_txt     = dataset_txt\n",
    "        self.targets_txt     = targets_txt\n",
    "        self.minutiae_txt    = minutiae_txt\n",
    "        self.eval_txt        = eval_txt\n",
    "        self.face_path       = face_paths\n",
    "        self.eval_path       = eval_paths\n",
    "        self.epoch_size    = epoch_size\n",
    "        self.batch_size    = batch_size\n",
    "        #self.target_dim = (160, 128)\n",
    "        self.target_dim = (320, 256)\n",
    "        #self.target_dim = (185, 128)\n",
    "        self.normalize  = True\n",
    "        self.threshold = False\n",
    "        \n",
    "        self.train_names   = []\n",
    "        self.train_targets_names = []\n",
    "        self.train_minutiae_names = []\n",
    "        self.test_names = []\n",
    "        self.test_targets_names = []\n",
    "        self.test_minutiae_names = []\n",
    "        self.eval_names = []\n",
    "        \n",
    "        with open(self.dataset_txt, 'r') as f:\n",
    "            self.dataset_fnames = [line.rstrip() for line in f]\n",
    "            \n",
    "        with open(self.targets_txt, 'r') as f:\n",
    "            self.targets_fnames = [line.rstrip() for line in f]    \n",
    "            \n",
    "        with open(self.minutiae_txt, 'r') as f:\n",
    "            self.minutiae_fnames = [line.rstrip() for line in f]\n",
    "            \n",
    "        with open(self.eval_txt, 'r') as f:\n",
    "            self.eval_fnames = [line.rstrip() for line in f]\n",
    "            \n",
    "        print(\"Loading dataset ...\")\n",
    "        sys.stdout.flush()\n",
    "        for i in np.arange(0, len(self.dataset_fnames) - int(len(self.dataset_fnames) - 100)):\n",
    "            self.train_names.append(os.path.join(self.face_path, self.dataset_fnames[i]))\n",
    "            self.train_targets_names.append(os.path.join(self.face_path, self.targets_fnames[i]))\n",
    "            self.train_minutiae_names.append(os.path.join(self.face_path, self.minutiae_fnames[i]))\n",
    "        for i in np.arange(int(len(self.dataset_fnames) - 100), len(self.dataset_fnames)):\n",
    "            self.test_names.append(os.path.join(self.face_path, self.dataset_fnames[i]))\n",
    "            self.test_targets_names.append(os.path.join(self.face_path, self.targets_fnames[i]))\n",
    "            self.test_minutiae_names.append(os.path.join(self.face_path, self.minutiae_fnames[i]))\n",
    "        for i in np.arange(0, len(self.eval_fnames)):\n",
    "            self.eval_names.append(os.path.join(self.eval_path, self.eval_fnames[i]))\n",
    "            \n",
    "        self.train_perm_idxs = np.random.permutation(len(self.train_names))\n",
    "        self.train_iter_cntr = 0\n",
    "        \n",
    "        self.fixed_index = None\n",
    "        \n",
    "    def enhance_image(self, img):\n",
    "        #img = skif.gaussian_filter(img, sigma = 0.05)\n",
    "        #img = exposure.equalize_adapthist(img, clip_limit = 0.03)\n",
    "        #img = img > 0.6#thresh\n",
    "        #img = skim.binary_closing(img, np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]]))\n",
    "        #img = skim.binary_dilation(img, np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]]))        \n",
    "        #img = skif.gaussian_filter(img, sigma = 0.05)\n",
    "        return img\n",
    "    \n",
    "    def modify_image(self, img):\n",
    "        img = skiu.random_noise(img, mode='speckle', seed=None, clip=True, mean = 0, var = 0.0035)\n",
    "        #for i in range(int(round(np.random.rand() * 2))):\n",
    "            #img = skim.erosion(img)\n",
    "            \n",
    "        return img\n",
    "    \n",
    "    def train_iter(self):\n",
    "        for i in xrange(self.epoch_size):\n",
    "            imgs, targets, masks, minutiaes = [], [], [], []\n",
    "            for b in xrange(self.batch_size):\n",
    "                self.train_iter_cntr += 1\n",
    "                if self.train_iter_cntr == len(self.train_names):\n",
    "                    self.train_perm_idxs = np.random.permutation(len(self.train_names))\n",
    "                    self.train_iter_cntr = 0\n",
    "                #idx = np.random.permutation(len(self.train_names))[0]\n",
    "                idx = self.train_perm_idxs[self.train_iter_cntr]\n",
    "                if self.fixed_index != None:\n",
    "                    idx = self.fixed_index\n",
    "        \n",
    "                minutiae = load_matlab_file(self.train_minutiae_names[idx], 'feats_mat').squeeze()\n",
    "        \n",
    "                img = rgb2gray(sp.misc.imread(self.train_names[idx]))\n",
    "                img = sp.misc.imresize(img, self.target_dim) \n",
    "                img = img_as_float(img)\n",
    "                img = self.modify_image(img)\n",
    "                #img = exposure.equalize_adapthist(img, clip_limit = 0.03)\n",
    "                \n",
    "                target = (sp.misc.imread(self.train_targets_names[idx]))\n",
    "                target = sp.misc.imresize(target, self.target_dim) \n",
    "                target = img_as_float(target)\n",
    "                target = self.enhance_image(target)\n",
    "\n",
    "                target = target.reshape((1, img.shape[0], img.shape[1]))\n",
    "                mask = np.zeros(img.shape)\n",
    "                img = img.reshape((1, img.shape[0], img.shape[1]))\n",
    "                mask = mask.reshape((1, mask.shape[0], mask.shape[1]))\n",
    "                minutiae = minutiae.reshape((1, minutiae.shape[0], minutiae.shape[1]))\n",
    "\n",
    "                imgs.append(img)\n",
    "                targets.append(target)\n",
    "                masks.append(mask)\n",
    "                minutiaes.append(minutiae)\n",
    "\n",
    "            imgs, targets, masks, minutiaes = np.array(imgs), np.array(targets), np.array(masks), np.array(minutiaes)\n",
    "            yield imgs, targets, masks, minutiaes\n",
    "                    \n",
    "    def train_iter_all(self):\n",
    "        for idx in xrange(len(self.train_names)):\n",
    "            minutiae = load_matlab_file(self.train_minutiae_names[idx], 'feats_mat').squeeze()\n",
    "                \n",
    "            img = rgb2gray(sp.misc.imread(self.train_names[idx]))\n",
    "            img = sp.misc.imresize(img, self.target_dim)\n",
    "            img = img_as_float(img)\n",
    "            img = self.modify_image(img)\n",
    "            #img = exposure.equalize_adapthist(img, clip_limit = 0.03)\n",
    "\n",
    "            target = (sp.misc.imread(self.train_targets_names[idx]))\n",
    "            target = sp.misc.imresize(target, self.target_dim) \n",
    "            target = img_as_float(target)\n",
    "            target = self.enhance_image(target)\n",
    "\n",
    "            target = target.reshape((1, 1, img.shape[0], img.shape[1]))\n",
    "            mask = np.zeros(img.shape)\n",
    "            img = img.reshape((1, 1, img.shape[0], img.shape[1]))\n",
    "            mask = mask.reshape((1, 1, mask.shape[0], mask.shape[1]))                \n",
    "            minutiae = minutiae.reshape((1, minutiae.shape[0], minutiae.shape[1]))\n",
    "\n",
    "            yield img, target, mask, minutiae\n",
    "            \n",
    "    def valid_iter(self):\n",
    "        idx = 0\n",
    "            \n",
    "        minutiae = load_matlab_file(self.train_minutiae_names[idx], 'feats_mat').squeeze()\n",
    "        \n",
    "        img = rgb2gray(sp.misc.imread(self.train_names[idx]))\n",
    "        img = sp.misc.imresize(img, self.target_dim)\n",
    "        img = img_as_float(img)\n",
    "        img = self.modify_image(img)\n",
    "        #img = exposure.equalize_adapthist(img, clip_limit = 0.03)\n",
    "                \n",
    "        target = (sp.misc.imread(self.train_targets_names[idx]))\n",
    "        target = sp.misc.imresize(target, self.target_dim) \n",
    "        target = img_as_float(target)\n",
    "        target = self.enhance_image(target)\n",
    "                \n",
    "        target = target.reshape((1, 1, img.shape[0], img.shape[1]))\n",
    "        mask = np.zeros(img.shape)\n",
    "        img = img.reshape((1, 1, img.shape[0], img.shape[1]))\n",
    "        mask = mask.reshape((1, 1, mask.shape[0], mask.shape[1]))                \n",
    "        minutiae = minutiae.reshape((1, minutiae.shape[0], minutiae.shape[1]))\n",
    "\n",
    "        yield img, target, mask\n",
    "        \n",
    "    def test_iter(self):\n",
    "        idx = np.random.permutation(len(self.test_names))[0]     \n",
    "        \n",
    "        minutiae = load_matlab_file(self.train_minutiae_names[idx], 'feats_mat').squeeze()   \n",
    "        \n",
    "        img = rgb2gray(sp.misc.imread(self.test_names[idx]))\n",
    "        img = sp.misc.imresize(img, self.target_dim)\n",
    "        img = img_as_float(img)\n",
    "        img = self.modify_image(img)\n",
    "        #img = exposure.equalize_adapthist(img, clip_limit = 0.03)\n",
    "        \n",
    "        target = (sp.misc.imread(self.test_targets_names[idx]))\n",
    "        target = sp.misc.imresize(target, self.target_dim) \n",
    "        target = img_as_float(target)\n",
    "        target = self.enhance_image(target)\n",
    "        \n",
    "        target = target.reshape((1, 1, img.shape[0], img.shape[1]))\n",
    "        mask = np.zeros(img.shape)\n",
    "        img = img.reshape((1, 1, img.shape[0], img.shape[1]))\n",
    "        mask = mask.reshape((1, 1, mask.shape[0], mask.shape[1]))                \n",
    "        minutiae = minutiae.reshape((1, minutiae.shape[0], minutiae.shape[1]))\n",
    "\n",
    "        yield img, target, mask\n",
    "\n",
    "    def test_iter_all(self):\n",
    "        for idx in xrange(len(self.test_names) // self.epoch_size):                \n",
    "            imgs, targets, masks, minutiaes = [], [], [], []\n",
    "            for i in xrange(self.epoch_size):\n",
    "                minutiae = load_matlab_file(self.train_minutiae_names[idx], 'feats_mat').squeeze()\n",
    "            \n",
    "                img = rgb2gray(sp.misc.imread(self.test_names[idx * self.epoch_size + i]))\n",
    "                img = sp.misc.imresize(img, self.target_dim)\n",
    "                img = img_as_float(img)\n",
    "                img = self.modify_image(img)\n",
    "                #img = exposure.equalize_adapthist(img, clip_limit = 0.03)\n",
    "        \n",
    "                target = (sp.misc.imread(self.test_targets_names[idx * self.epoch_size + i]))\n",
    "                target = sp.misc.imresize(target, self.target_dim) \n",
    "                target = img_as_float(target)\n",
    "                target = self.enhance_image(target)\n",
    "                \n",
    "                target = target.reshape((1, img.shape[0], img.shape[1]))\n",
    "                mask = np.zeros(img.shape)\n",
    "                img = img.reshape((1, img.shape[0], img.shape[1]))\n",
    "                mask = mask.reshape((1, mask.shape[0], mask.shape[1]))                \n",
    "                minutiae = minutiae.reshape((1, minutiae.shape[0], minutiae.shape[1]))\n",
    "\n",
    "                imgs.append(img)\n",
    "                targets.append(target)\n",
    "                masks.append(mask)\n",
    "                minutiaes.append(minutiae)\n",
    "\n",
    "            imgs, targets, masks, minutiaes = np.array(imgs), np.array(targets), np.array(masks), np.array(minutiaes)\n",
    "            yield imgs, targets, masks, minutiaes\n",
    "              \n",
    "    def eval_iter_all(self):\n",
    "        for idx in xrange(len(self.eval_names) // self.epoch_size + 1):                \n",
    "            imgs, targets, masks, names = [], [], [], []\n",
    "            for i in xrange(self.epoch_size):\n",
    "                if idx * self.epoch_size + i >= len(self.eval_names):\n",
    "                    break\n",
    "                    \n",
    "                #img = rgb2gray(sp.misc.imread(self.eval_names[idx * self.epoch_size + i]))\n",
    "                img = sp.misc.imread(self.eval_names[idx * self.epoch_size + i])\n",
    "                img = sp.misc.imresize(img, self.target_dim)\n",
    "               \n",
    "                img = img_as_float(img)\n",
    "                padsz = 60\n",
    "                padsz2 = padsz // 2\n",
    "                imgg = np.ones((img.shape[0] + padsz, img.shape[1] + padsz))\n",
    "                imgg[padsz2:img.shape[0]+padsz2, padsz2:img.shape[1]+padsz2] = img\n",
    "                img = imgg#sp.misc.imresize(imgg, self.target_dim)\n",
    "                \n",
    "                #img = self.modify_image(img)\n",
    "                img = exposure.equalize_adapthist(img, clip_limit = 0.03)\n",
    "                #img = img > 0.5\n",
    "                \n",
    "                #img = exposure.equalize_adapthist(img, clip_limit = 0.025)\n",
    "                #img = exposure.equalize_adapthist(img, clip_limit = 0.005)\n",
    "                \n",
    "                target = img.copy().reshape((1, img.shape[0], img.shape[1]))\n",
    "                #target = target > 0.99\n",
    "                #target = skim.binary_closing(target, np.ones((3, 3)))\n",
    "                #img, mask = self.random_corrupt3(img)\n",
    "                #img, mask = self.random_corrupt(img)\n",
    "                #img, mask = self.random_corrupt3(img)\n",
    "                mask = np.zeros(img.shape)\n",
    "                img = img.reshape((1, img.shape[0], img.shape[1]))\n",
    "                mask = mask.reshape((1, mask.shape[0], mask.shape[1]))\n",
    "\n",
    "                imgs.append(img)\n",
    "                targets.append(target)\n",
    "                masks.append(mask)\n",
    "                names.append(self.eval_names[idx * self.epoch_size + i])\n",
    "\n",
    "            imgs, targets, masks = np.array(imgs), np.array(targets), np.array(masks)            \n",
    "            yield imgs, targets, masks, names\n",
    "            \n",
    "tic = time.time()\n",
    "ds = FaceDataset(os.path.join(cwd_path, 'dataset.txt'), \n",
    "                 os.path.join(cwd_path, 'targets.txt'),\n",
    "                 os.path.join(cwd_path, 'minutiae.txt'),\n",
    "                  os.path.join(data_path, 'fingerprints_newdata/'),\n",
    "                  os.path.join(cwd_path, 'eval.txt'),\n",
    "                  os.path.join(data_path, 'fingerprints_eval/'))\n",
    "print('Loading done! (duration: {0} seconds)'.format(time.time() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Code for debugging purposes, for easily visualizing the data loaded by the dataset\n",
    "'''\n",
    "for te_train, te_target, te_mask, names in ds.eval_iter_all():\n",
    "    te_out, te_ta = ae_encode(input_transform(te_train), target_transform(te_target))\n",
    "    te_out, te_ta = ae_encode(te_out, te_ta)\n",
    "    te_out, te_ta = ae_encode(te_out, te_ta)\n",
    "    te_out, te_ta = ae_encode(te_out, te_ta)\n",
    "    #te_out, te_ta = ae_encode(te_out, te_ta)\n",
    "    #te_out, te_ta = ae_encode(te_out, te_ta)\n",
    "\n",
    "    te_reshape = inverse(te_out)\n",
    "    te_target_reshape = inverse(te_ta)\n",
    "    te_train_reshape = inverse(input_transform(te_train))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(te_train[0, 0, :, :], 'gray')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(te_reshape[0, 0, :, :], 'gray')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(te_target_reshape[0, 0, :, :], 'gray')\n",
    "    \n",
    "    #diffimg = np.abs(te_reshape[0, 0, :, :] - te_target_reshape[0, 0, :, :])\n",
    "    #plt.figure()\n",
    "    #plt.imshow(diffimg)\n",
    "    \n",
    "    #print np.min(diffimg.flatten())\n",
    "    #print np.max(diffimg.flatten())\n",
    "    #print np.mean(diffimg.flatten())\n",
    "    \n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Setup Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def target_transform(X):\n",
    "    return floatX(X).transpose(0, 1, 3, 2)\n",
    "\n",
    "def input_transform(X):\n",
    "    return target_transform(X)\n",
    "\n",
    "def mask_transform(X):\n",
    "    return X.transpose(0, 1, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l2 = 1e-4        # l2 weight decay\n",
    "b1 = 0.5          # momentum term of adam\n",
    "nc = 1#3            # # of channels in image\n",
    "lr = 0.0002       # initial learning rate for adam\n",
    "\n",
    "relu = activations.Rectify()\n",
    "sigmoid = activations.Sigmoid()\n",
    "lrelu = activations.LeakyRectify()\n",
    "tanh = activations.Tanh()\n",
    "bce = T.nnet.binary_crossentropy\n",
    "\n",
    "def wmse(x,y, weights):\n",
    "    return T.sum(T.pow(x-y,2) * weights, axis = 1)\n",
    "\n",
    "def mse(x,y):\n",
    "    return T.sum(T.pow(x-y,2), axis = 1)\n",
    "\n",
    "def mmse(x,y,m):\n",
    "    return T.sum(T.pow(x-y,2)*m, axis = 1) / T.sum(T.gt(m, 1e-6), axis = 1, dtype = theano.config.floatX)\n",
    "\n",
    "gifn = inits.Normal(scale=0.02)\n",
    "difn = inits.Normal(scale=0.02)\n",
    "sigma_ifn = inits.Normal(loc = -100., scale=0.02)\n",
    "gain_ifn = inits.Normal(loc=1., scale=0.02)\n",
    "bias_ifn = inits.Constant(c=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The following methods are to help adjust the sizes of the convolutional layers in the generator and discriminator, \n",
    "which is very fiddly to do otherwise.  The (overloaded) method make_conv_set can be used to create both the conv \n",
    "and deconv sets of layers.  Note that the 'size' of the images is the size of the shortest side (32 in the input set, \n",
    "128 in the target set).  Only use powers of 2 here.\n",
    "'''\n",
    "def make_conv_layer(X, input_size, output_size, input_filters, \n",
    "                    output_filters, name, index,\n",
    "                    weights = None, filter_sz = 5):\n",
    "    \n",
    "    is_deconv = output_size >= input_size\n",
    "\n",
    "    w_size = (input_filters, output_filters, filter_sz, filter_sz) \\\n",
    "            if is_deconv else (output_filters, input_filters, filter_sz, filter_sz)\n",
    "    \n",
    "    if weights is None:\n",
    "        w = gifn(w_size, '%sw%i' %(name, index))\n",
    "        g = gain_ifn((output_filters), '%sg%i' %(name, index))\n",
    "        b = bias_ifn((output_filters), '%sb%i' %(name, index))\n",
    "    else:\n",
    "        w,g,b = weights\n",
    "        \n",
    "    print w_size\n",
    "    \n",
    "    conv_method = deconv if is_deconv else dnn_conv\n",
    "    activation = relu if is_deconv else lrelu\n",
    "        \n",
    "    sub = output_size / input_size if is_deconv else input_size / output_size\n",
    "    \n",
    "    bm = filter_sz // 2\n",
    "    #if filter_sz == 3:\n",
    "    #    bm = 1\n",
    "    #else:\n",
    "    #    bm = 2\n",
    "    \n",
    "    layer = activation(batchnorm(conv_method(X, w, subsample=(sub, sub), border_mode=(bm, bm)), g=g, b=b))\n",
    "    \n",
    "    return layer, [w,g,b]\n",
    "\n",
    "def make_conv_set(input, layer_sizes, num_filters, name, weights = None, filter_szs = None):\n",
    "    assert(len(layer_sizes) == len(num_filters))\n",
    "    \n",
    "    vars_ = []\n",
    "    layers_ = []\n",
    "    current_layer = input\n",
    "    \n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        input_size = layer_sizes[i]\n",
    "        output_size = layer_sizes[i + 1]\n",
    "        input_filters = num_filters[i]\n",
    "        output_filters = num_filters[i + 1]\n",
    "        \n",
    "        if weights is not None:\n",
    "            this_wts = weights[i * 3 : i * 3 + 3]\n",
    "        else:\n",
    "            this_wts = None\n",
    "            \n",
    "        if filter_szs != None:\n",
    "            filter_sz = filter_szs[i]\n",
    "        else:\n",
    "            filter_sz = 5\n",
    "        \n",
    "        layer, new_vars = make_conv_layer(current_layer, input_size, output_size, \n",
    "                                          input_filters, output_filters, name, i, \n",
    "                                          weights = this_wts, filter_sz = filter_sz)\n",
    "        \n",
    "        vars_ += new_vars\n",
    "        layers_ += [layer]\n",
    "        current_layer = layer\n",
    "        \n",
    "    return current_layer, vars_, layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "####\n",
    "# Use code below if you want use a saved model\n",
    "####\n",
    "'''\n",
    "[e_params, g_params] = pickle.load( open( \"model_params_350.pkl\", \"rb\" ) )\n",
    "gwx = g_params[-1]\n",
    "\n",
    "# inputs\n",
    "X = T.tensor4()\n",
    "# masks\n",
    "minutiae = T.tensor4()\n",
    "\n",
    "## encode layer\n",
    "e_layer_sizes = [256, 128, 64, 32, 16, 8]\n",
    "e_filter_sizes = [1, 128, 256, 256, 512, 1024]\n",
    "e_fszs = [13, 11, 9, 7, 5]\n",
    "\n",
    "eX, e_params, e_layers = make_conv_set(X, e_layer_sizes, e_filter_sizes, \"e\", weights=e_params, filter_szs = e_fszs)\n",
    "\n",
    "## generative layer\n",
    "g_layer_sizes = [8, 16, 32, 64, 128, 256]\n",
    "g_num_filters = [1024, 512, 256, 256, 128, 128]\n",
    "g_fszs = [5, 7, 9, 11, 13]\n",
    "\n",
    "g_out, g_params, g_layers = make_conv_set(eX, g_layer_sizes, g_num_filters, \"g\", weights=g_params, filter_szs = g_fszs)\n",
    "g_params += [gwx]\n",
    "gX = sigmoid(deconv(g_out, gwx, subsample=(1, 1), border_mode=(2, 2)))\n",
    "\n",
    "# target outputs\n",
    "target = T.tensor4()\n",
    "'''\n",
    "\n",
    "####\n",
    "#Use code below if you are training a model from scratch\n",
    "####\n",
    "# inputs\n",
    "X = T.tensor4()\n",
    "# masks\n",
    "minutiae = T.tensor4()\n",
    "\n",
    "## encode layer\n",
    "e_layer_sizes = [256, 128, 64, 32, 16, 8]\n",
    "e_filter_sizes = [1, 128, 256, 256, 512, 1024]\n",
    "e_fszs = [13, 11, 9, 7, 5]\n",
    "\n",
    "eX, e_params, e_layers = make_conv_set(X, e_layer_sizes, e_filter_sizes, \"e\", filter_szs = e_fszs)\n",
    "\n",
    "## generative layer\n",
    "g_layer_sizes = [8, 16, 32, 64, 128, 256]\n",
    "g_num_filters = [1024, 512, 256, 256, 128, 128]\n",
    "g_fszs = [5, 7, 9, 11, 13]\n",
    "\n",
    "g_out, g_params, g_layers = make_conv_set(eX, g_layer_sizes, g_num_filters, \"g\", filter_szs = g_fszs)\n",
    "gwx = gifn((128, nc, 5, 5), 'gwx')\n",
    "g_params += [gwx]\n",
    "gX = sigmoid(deconv(g_out, gwx, subsample=(1, 1), border_mode=(2, 2)))\n",
    "\n",
    "# target outputs\n",
    "target = T.tensor4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.ndimage.filters as fi\n",
    "\n",
    "def gkern2(kernlen=21, nsig=3):\n",
    "    \"\"\"Returns a 2D Gaussian kernel array.\"\"\"\n",
    "\n",
    "    # create nxn zeros\n",
    "    inp = np.zeros((kernlen, kernlen))\n",
    "    # set element at the middle to one, a dirac delta\n",
    "    inp[kernlen//2, kernlen//2] = 1\n",
    "    # gaussian-smooth the dirac, resulting in a gaussian filter mask\n",
    "    return fi.gaussian_filter(inp, nsig)\n",
    "\n",
    "def ridge_orientation(img):\n",
    "    # Compute ridge orientations\n",
    "    # Precomputed gaussian\n",
    "    gauss_block = theano.shared(np.array([[gkern2(7, 1)]],\n",
    "                dtype = theano.config.floatX), name ='gauss_block')\n",
    "    gauss_orient = theano.shared(np.array([[gkern2(31, 5)]],\n",
    "                dtype = theano.config.floatX), name ='gauss_orient')\n",
    "    dx = theano.shared(np.array([[[[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]]],\n",
    "                dtype = theano.config.floatX), name ='dx')\n",
    "    dy = theano.shared(np.array([[[[-1, -1, -1], [0, 0, 0], [1, 1, 1]]]],\n",
    "            dtype = theano.config.floatX), name ='dy')\n",
    "    # Get gaussian gradient\n",
    "    gfx = T.nnet.conv2d(gauss_block, dx, border_mode = 'half')\n",
    "    gfy = T.nnet.conv2d(gauss_block, dy, border_mode = 'half')\n",
    "    # Get image gradients\n",
    "    gfx_img = T.nnet.conv2d(img, gfx, border_mode = 'half')\n",
    "    gfy_img = T.nnet.conv2d(img, gfy, border_mode = 'half')\n",
    "    # Covariance of the data for image gradients\n",
    "    gfxx_img, gfxy_img, gfyy_img = gfx_img**2, gfx_img * gfy_img, gfy_img**2\n",
    "\n",
    "    # Smooth the covariance data\n",
    "    gfxx_img = T.nnet.conv2d(gfxx_img, gauss_orient, border_mode = 'half')\n",
    "    gfxy_img = 2 * T.nnet.conv2d(gfxy_img, gauss_orient, border_mode = 'half')\n",
    "    gfyy_img = T.nnet.conv2d(gfyy_img, gauss_orient, border_mode = 'half')\n",
    "\n",
    "    # Analytic solution of the principal direction\n",
    "    denom = T.sqrt(gfxy_img**2 + (gfxx_img - gfyy_img)**2 + 1e-12)\n",
    "    sin2theta = gfxy_img / denom\n",
    "    cos2theta = (gfxx_img - gfyy_img) / denom\n",
    "\n",
    "    # Smooth the double angles\n",
    "    sin2theta = T.nnet.conv2d(sin2theta, gauss_orient, border_mode = 'half')\n",
    "    cos2theta = T.nnet.conv2d(cos2theta, gauss_orient, border_mode = 'half')\n",
    "\n",
    "    # Compute the final orientation\n",
    "    orient_img = np.pi / 2.0 + T.arctan2(sin2theta, cos2theta) / 2.0\n",
    "    \n",
    "    # Reliability and coherence\n",
    "    Imin = (gfyy_img + gfxx_img) / 2.0 - (gfxx_img - gfyy_img) * cos2theta / 2.0 - gfxy_img * sin2theta / 2.0\n",
    "    Imax = gfyy_img + gfxx_img - Imin\n",
    "    \n",
    "    reliability = 1 - Imin / (Imax + 0.001)\n",
    "    reliability = T.switch(T.gt(denom, 0.001), reliability, 0)\n",
    "    coherence = ((Imax - Imin) / (Imax + Imin + 1e-12))**2\n",
    "    \n",
    "    return orient_img, reliability, coherence\n",
    "\n",
    "imgg = T.ftensor4('imgg')\n",
    "output, rel, coh = ridge_orientation(imgg)\n",
    "\n",
    "forient = theano.function([imgg],\n",
    "                            [output],\n",
    "                            allow_input_downcast = True)\n",
    "\n",
    "def ridge_gradient(img):\n",
    "    dx = theano.shared(np.array([[[[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]]],\n",
    "            dtype = theano.config.floatX), name ='dx')\n",
    "    dy = theano.shared(np.array([[[[-1, -1, -1], [0, 0, 0], [1, 1, 1]]]],\n",
    "                dtype = theano.config.floatX), name ='dy')\n",
    "    d45 = theano.shared(np.array([[[[-1, -1, 0], [-1, 0, 1], [0, 1, 1]]]],\n",
    "                dtype = theano.config.floatX), name = 'd45')\n",
    "    d135 = theano.shared(np.array([[[[0, -1, -1], [1, 0, -1], [1, 1, 0]]]],\n",
    "                dtype = theano.config.floatX), name = 'd135')\n",
    "\n",
    "    # Compute gradient of the target\n",
    "    gx_img = T.nnet.conv2d(img, dx, border_mode = 'half')\n",
    "    gy_img = T.nnet.conv2d(img, dy, border_mode = 'half')\n",
    "    g45_img = T.nnet.conv2d(img, d45, border_mode = 'half')\n",
    "    g135_img = T.nnet.conv2d(img, d135, border_mode = 'half')\n",
    "    \n",
    "    return gx_img, gy_img, g45_img, g135_img\n",
    "\n",
    "def dilate_minutiae(img):\n",
    "    g = theano.shared(np.array([[np.ones((5, 5))]],\n",
    "                dtype = theano.config.floatX), name ='g')\n",
    "    img = img.dimshuffle(0, 'x', 1, 2)\n",
    "    img = T.nnet.conv2d(img, g, border_mode = 'half')\n",
    "    img = T.gt(img, 0) \n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Cost functions\n",
    "from theano.tensor.signal.pool import pool_2d\n",
    "\n",
    "## MSE encoding cost is done on an (averaged) downscaling of the image\n",
    "target_pool = target\n",
    "target_flat = T.flatten(target_pool, 2)\n",
    "gX_pool = gX\n",
    "gX_flat = T.flatten(gX_pool,2)\n",
    "\n",
    "# Compute target minutiae based weights\n",
    "#weights = T.switch(T.gt(minutiae, 0), 1, 1)\n",
    "#weights = weights.flatten(2)\n",
    "\n",
    "gx_target, gy_target, g45_target, g135_target = ridge_gradient(target_pool)\n",
    "o_target, r_target, c_target = ridge_orientation(target_pool)\n",
    "#o_target = pool_2d(o_target, (4,4), mode=\"average_exc_pad\",ignore_border=True)\n",
    "\n",
    "# Compute gradient of the output\n",
    "gx_X, gy_X, g45_X, g135_X = ridge_gradient(gX_pool)\n",
    "o_X, r_X, c_X = ridge_orientation(gX_pool)\n",
    "#o_X = pool_2d(o_X, (4,4), mode=\"average_exc_pad\",ignore_border=True)\n",
    "\n",
    "enc_cost_gx = mse(gx_X.flatten(2), gx_target.flatten(2))\n",
    "enc_cost_gy = mse(gy_X.flatten(2), gy_target.flatten(2))\n",
    "enc_cost_g45 = mse(g45_X.flatten(2), g45_target.flatten(2))\n",
    "enc_cost_g135 = mse(g135_X.flatten(2), g135_target.flatten(2))\n",
    "enc_cost_o = mse(o_X.flatten(2), o_target.flatten(2))\n",
    "enc_cost_r = mse(r_X.flatten(2), r_target.flatten(2))\n",
    "\n",
    "# Final encoding cost\n",
    "enc_cost_grads = (enc_cost_gx + enc_cost_gy + enc_cost_g45 + enc_cost_g135) / 4.0\n",
    "enc_cost_ori = enc_cost_o / 10.0\n",
    "enc_cost_rel = enc_cost_r / 10.0\n",
    "enc_cost = enc_cost_grads + enc_cost_ori + enc_cost_rel\n",
    "enc_cost = enc_cost.mean(dtype = theano.config.floatX)\n",
    "\n",
    "#To change the weight of MSE, change the denominator.  ex. enc_cost/5 weights MSE much less that enc_cost/1\n",
    "g_cost = enc_cost\n",
    "\n",
    "## N.B. e_cost and e_updates will only try and minimise MSE loss on the autoencoder (for debugging)\n",
    "e_cost = enc_cost\n",
    "\n",
    "cost = [e_cost, g_cost, enc_cost_grads.mean(dtype = theano.config.floatX) , enc_cost_ori.mean(dtype = theano.config.floatX) , enc_cost_rel.mean(dtype = theano.config.floatX), enc_cost_rel.mean(dtype = theano.config.floatX)]\n",
    "\n",
    "elrt = sharedX(0.002)\n",
    "lrt = sharedX(lr)\n",
    "g_updater = updates.Adam(lr=lrt, b1=b1, regularizer=updates.Regularizer(l2=l2))\n",
    "e_updater = updates.Adam(lr=elrt, b1=b1, regularizer=updates.Regularizer(l2=l2))\n",
    "\n",
    "g_updates = g_updater(e_params + g_params, g_cost)\n",
    "e_updates = e_updater(e_params, e_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print 'COMPILING'\n",
    "t = time.time()\n",
    "_train_g = theano.function([X, target, minutiae], cost, updates=g_updates, allow_input_downcast = True, on_unused_input='warn')\n",
    "_train_e = theano.function([X, target, minutiae], cost, updates=e_updates, allow_input_downcast = True, on_unused_input='warn')\n",
    "_get_cost = theano.function([X, target, minutiae], cost, allow_input_downcast = True, on_unused_input='warn')\n",
    "print '%.2f seconds to compile theano functions'%(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Testing and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "img_dir = \"gen_images/\"\n",
    "\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)\n",
    "    os.makedirs(os.path.join(img_dir, \"tests\"))\n",
    "    \n",
    "eval_dir = 'evals'\n",
    "if not os.path.exists(os.path.join(img_dir, eval_dir)):\n",
    "    os.makedirs(os.path.join(img_dir, eval_dir))\n",
    "\n",
    "ae_encode = theano.function([X, target], [gX, target])\n",
    "\n",
    "def inverse(X):\n",
    "    X_pred = X.transpose(0, 1, 3, 2)\n",
    "    X_pred = (X_pred) * 255.0\n",
    "    #X_pred = (X + 1) * 127.5\n",
    "    X_pred = np.rint(X_pred).astype(int)\n",
    "    X_pred = np.clip(X_pred, a_min = 0, a_max = 255)\n",
    "    return X_pred.astype('uint8')\n",
    "\n",
    "\n",
    "def save_sample_pictures():\n",
    "    for te_train, te_target, te_mask, te_minutiae in ds.train_iter():\n",
    "        break\n",
    "    te_out, te_ta = ae_encode(input_transform(te_train), target_transform(te_target))\n",
    "    te_reshape = inverse(te_out)\n",
    "    te_target_reshape = inverse(te_ta)\n",
    "    te_train_reshape = inverse(input_transform(te_train))\n",
    "    \n",
    "    new_size = (ds.target_dim[1] * 6, ds.target_dim[0] * 12)\n",
    "    new_im = Image.new('L', new_size)\n",
    "    r = np.random.choice(12, 24, replace=True).reshape(2,12)\n",
    "    for i in range(2):\n",
    "        for j in range(12):\n",
    "            index =  r[i][j]\n",
    "            \n",
    "            target_im = Image.fromarray(te_target_reshape[index][0])\n",
    "            train_im = Image.fromarray(te_train_reshape[index][0])\n",
    "            im = Image.fromarray(te_reshape[index][0])\n",
    "            \n",
    "            new_im.paste(target_im, (ds.target_dim[1] * i * 3, ds.target_dim[0] * j))\n",
    "            new_im.paste(train_im, (ds.target_dim[1] * (i * 3 + 1), ds.target_dim[0] * j))\n",
    "            new_im.paste(im, (ds.target_dim[1] * (i * 3 + 2), ds.target_dim[0] * j))\n",
    "            \n",
    "    img_loc = \"%s/%i.png\" % (img_dir, int(time.time()))    \n",
    "    print \"saving images to %s\" %img_loc\n",
    "    new_im.save(img_loc)\n",
    "\n",
    "#saves output for all testing images.  This may take a couple of minutes to run.\n",
    "def save_all_pictures():\n",
    "    counter = 0\n",
    "    counter2 = 0\n",
    "    for te_train, te_target, te_mask, te_minutiae in ds.test_iter_all():\n",
    "        te_out, te_ta = ae_encode(input_transform(te_train), target_transform(te_target))\n",
    "        te_reshape = inverse(te_out)\n",
    "        te_target_reshape = inverse(te_ta)\n",
    "        te_train_reshape = inverse(input_transform(te_train))\n",
    "\n",
    "        new_size = (ds.target_dim[1] * 3, ds.target_dim[0] * 12)\n",
    "        new_im = Image.new('L', new_size)\n",
    "        r = [range(12),range(12)]\n",
    "        for i in range(1):\n",
    "            for j in range(12):\n",
    "                index =  r[i][j]\n",
    "                try:\n",
    "                    target_im = Image.fromarray(te_target_reshape[index][0])\n",
    "                    train_im = Image.fromarray(te_train_reshape[index][0])\n",
    "                    im = Image.fromarray(te_reshape[index][0])\n",
    "                    \n",
    "                    #scale = 1.6\n",
    "                    #target_im.thumbnail((ds.target_dim[1] * scale, ds.target_dim[0] * scale), Image.ANTIALIAS) \n",
    "                    #train_im.thumbnail((ds.target_dim[1] * scale, ds.target_dim[0] * scale), Image.ANTIALIAS) \n",
    "                    #im.thumbnail((ds.target_dim[1] * scale, ds.target_dim[0] * scale), Image.ANTIALIAS) \n",
    "                    \n",
    "                    #tgt_sz = (512, 480)\n",
    "                    \n",
    "                    tgt_im = Image.new('L', (ds.target_dim[1], ds.target_dim[0]))\n",
    "                    tgt_im.paste(target_im, (0, 0))        \n",
    "                    #tgt_im.thumbnail((ds.target_dim[1] * 2, ds.target_dim[0] * 2), Image.ANTIALIAS)            \n",
    "                    img_loc = \"%s/tests/test_tgt_%i.png\" % (img_dir, counter2)       \n",
    "                    tgt_im.save(img_loc)  \n",
    "\n",
    "                    in_im = Image.new('L', (ds.target_dim[1], ds.target_dim[0]))\n",
    "                    in_im.paste(train_im, (0, 0))                    \n",
    "                    #in_im.thumbnail((ds.target_dim[1] * 2, ds.target_dim[0] * 2), Image.ANTIALIAS)\n",
    "                    img_loc = \"%s/tests/test_in_%i.png\" % (img_dir, counter2)       \n",
    "                    in_im.save(img_loc)  \n",
    "                    \n",
    "                    res_im = Image.new('L', (ds.target_dim[1], ds.target_dim[0]))\n",
    "                    res_im.paste(im, (0, 0))\n",
    "                    #res_im.thumbnail((ds.target_dim[1] * 2, ds.target_dim[0] * 2), Image.ANTIALIAS)\n",
    "                    res_im.thumbnail((ds.target_dim[1] * 1.6, ds.target_dim[0] * 1.6), Image.ANTIALIAS)\n",
    "                    img_loc = \"%s/tests/test_pred_%i.png\" % (img_dir, counter2)   \n",
    "                    res_im.save(img_loc)\n",
    "                    \n",
    "                    counter2+=1\n",
    "                    \n",
    "                    new_im.paste(target_im, (ds.target_dim[1] * i * 3, ds.target_dim[0] * j))\n",
    "                    new_im.paste(train_im, (ds.target_dim[1] * (i * 3 + 1), ds.target_dim[0] * j))\n",
    "                    new_im.paste(im, (ds.target_dim[1] * (i * 3 + 2), ds.target_dim[0] * j))\n",
    "                except:\n",
    "                    print \"Eror with training image\"\n",
    "        img_loc = \"%s/test_result_%i.png\" % (img_dir, counter)   \n",
    "        print \"saving images to %s\" %img_loc\n",
    "        new_im.save(img_loc)\n",
    "        counter+=1\n",
    "        \n",
    "#saves output for all testing images.  This may take a couple of minutes to run.\n",
    "def eval_all_pictures():\n",
    "    counter = 0\n",
    "    for te_train, te_target, te_mask, names in ds.eval_iter_all():\n",
    "        te_out, te_ta = ae_encode(input_transform(te_train), target_transform(te_target))\n",
    "        te_out, te_ta = ae_encode(te_out, te_ta)\n",
    "        te_out, te_ta = ae_encode(te_out, te_ta)\n",
    "        te_out, te_ta = ae_encode(te_out, te_ta)\n",
    "        te_reshape = inverse(te_out)\n",
    "        te_target_reshape = inverse(te_ta)\n",
    "        te_train_reshape = inverse(input_transform(te_train))\n",
    "\n",
    "        new_size = (ds.target_dim[1] * 3, ds.target_dim[0] * 12)\n",
    "        new_im = Image.new('L', new_size)\n",
    "        r = [range(12),range(12)]\n",
    "        for i in range(1):\n",
    "            for j in range(12):\n",
    "                index =  r[i][j]\n",
    "                try:\n",
    "                    target_im = Image.fromarray(te_target_reshape[index][0])\n",
    "                    train_im = Image.fromarray(te_train_reshape[index][0])\n",
    "                    im = Image.fromarray(te_reshape[index][0])\n",
    "\n",
    "                    tgt_im = Image.new('L', (ds.target_dim[1], ds.target_dim[0]))\n",
    "                    tgt_im.paste(target_im, (0, 0))                    \n",
    "                    img_loc = \"%s/%s/eval_tgt_%s.png\" % (img_dir, eval_dir, os.path.splitext(os.path.split(names[j])[1])[0])       \n",
    "                    tgt_im.save(img_loc)  \n",
    "\n",
    "                    in_im = Image.new('L', (ds.target_dim[1], ds.target_dim[0]))\n",
    "                    in_im.paste(train_im, (0, 0))                    \n",
    "                    img_loc = \"%s/%s/eval_in_%s.png\" % (img_dir, eval_dir, os.path.splitext(os.path.split(names[j])[1])[0])       \n",
    "                    in_im.save(img_loc)  \n",
    "                    \n",
    "                    res_im = Image.new('L', (ds.target_dim[1], ds.target_dim[0]))\n",
    "                    res_im.paste(im, (0, 0))\n",
    "                    img_loc = \"%s/%s/eval_pred_%s.png\" % (img_dir, eval_dir, os.path.splitext(os.path.split(names[j])[1])[0])   \n",
    "                    res_im.save(img_loc)\n",
    "                    \n",
    "                    new_im.paste(target_im, (ds.target_dim[1] * i * 3, ds.target_dim[0] * j))\n",
    "                    new_im.paste(train_im, (ds.target_dim[1] * (i * 3 + 1), ds.target_dim[0] * j))\n",
    "                    new_im.paste(im, (ds.target_dim[1] * (i * 3 + 2), ds.target_dim[0] * j))\n",
    "                except:\n",
    "                    print \"Eror with training image\"\n",
    "        img_loc = \"%s/eval_result_%i.png\" % (img_dir, counter)     \n",
    "        print \"saving images to %s\" %img_loc\n",
    "        new_im.save(img_loc)\n",
    "        counter+=1\n",
    "\n",
    "#save_all_pictures()        \n",
    "#save_sample_pictures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ds.epoch_size = 12\n",
    "save_all_pictures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# No eval pictures available for this code example\n",
    "#ds.epoch_size = 12\n",
    "#eval_all_pictures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mn(l):\n",
    "    if sum(l) == 0:\n",
    "        return 0\n",
    "    return sum(l) / len(l)\n",
    "\n",
    "## TODO : nicer way of coding these means?\n",
    "\n",
    "def get_test_errors():\n",
    "    print \"getting test error\"\n",
    "    e_costs = []\n",
    "    g_costs = []\n",
    "    e_costs_grad = []\n",
    "    e_costs_ori = []\n",
    "    e_costs_rel = []\n",
    "    for i in range(20):\n",
    "        for samples in ds.valid_iter():\n",
    "            break\n",
    "        x_train, x_target, x_mask = samples\n",
    "        x = input_transform(x_train)\n",
    "        t = target_transform(x_target)\n",
    "        m = mask_transform(x_mask)\n",
    "        cost = _get_cost(x,t,m)\n",
    "        e_cost, g_cost, enc_cost_grad, enc_cost_ori, enc_cost_rel, _ = cost\n",
    "        e_costs.append(e_cost)\n",
    "        g_costs.append(g_cost)\n",
    "        e_costs_grad.append(enc_cost_grad)\n",
    "        e_costs_ori.append(enc_cost_ori)\n",
    "        e_costs_rel.append(enc_cost_rel)\n",
    "        \n",
    "    s = '[TEST] enc: %3.2f, gen: %3.2f, mse_grad: %3.2f, mse_ori: %3.2f, mse_rel: %3.2f' % (mn(e_costs), mn(g_costs), mn(e_costs_grad), mn(e_costs_ori), mn(e_costs_rel))\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train Model\n",
    "\n",
    "Finally, we come to the actual training of the model.  This code can be keyboard interrupted, and the weights will be stored in memory, allowing us to stop, adjust and restart the training (this is how I got the model to train).  For advice on training see the blog post at (#TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#iterator = tr_stream.get_epoch_iterator()\n",
    "\n",
    "# you may wish to reset the learning rate to something of your choosing if you feel it is too high/low\n",
    "lrt = sharedX(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "theano.config.exception_verbosity = 'low'\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "n_updates = 0\n",
    "t = time.time()\n",
    "\n",
    "n_epochs = 350#1000\n",
    "\n",
    "print \"STARTING\"\n",
    "\n",
    "for epoch in range(301, n_epochs):\n",
    "    tm = time.time()\n",
    "\n",
    "    e_costs = []\n",
    "    g_costs = []\n",
    "    e_costs_grad = []\n",
    "    e_costs_ori = []\n",
    "    e_costs_rel = []\n",
    "            \n",
    "    ## TODO : produces pretty ugly output, redo this?\n",
    "    for i in range(64):\n",
    "        for samples in ds.train_iter():\n",
    "            break\n",
    "        x_train, x_target, x_mask, x_minutiae = samples\n",
    "        x = input_transform(x_train)\n",
    "        t = target_transform(x_target)\n",
    "        #m = mask_transform(x_mask)\n",
    "        m = mask_transform(x_minutiae)\n",
    "        \n",
    "        cost = _train_g(x,t,m) \n",
    "        e_cost, g_cost, enc_cost_grad, enc_cost_ori, enc_cost_rel, _ = cost\n",
    "        e_costs.append(e_cost)\n",
    "        g_costs.append(g_cost)\n",
    "        e_costs_grad.append(enc_cost_grad)\n",
    "        e_costs_ori.append(enc_cost_ori)\n",
    "        e_costs_rel.append(enc_cost_rel)\n",
    "\n",
    "        if n_updates % 100 == 0:\n",
    "            s = '[TRAIN] enc: %3.2f, gen: %3.2f, mse_grad: %3.2f, mse_ori: %3.2f, mse_rel: %3.2f' % (mn(e_costs), mn(g_costs), mn(e_costs_grad), mn(e_costs_ori), mn(e_costs_rel))\n",
    "            e_costs = []\n",
    "            g_costs = []\n",
    "            e_costs_grad = []\n",
    "            e_costs_ori = []\n",
    "            e_costs_rel = []\n",
    "            logger.info(get_test_errors())\n",
    "            logger.info(s)\n",
    "            sys.stdout.flush()\n",
    "            save_sample_pictures()\n",
    "        n_updates += 1  \n",
    "\n",
    "    logger.info(\"Epoch %i of %i took %.2f seconds.\", epoch, n_epochs, time.time() - tm)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        all_params = [e_params, g_params]\n",
    "        pickle.dump(all_params, open(\"model_params_%d.pkl\" % epoch, 'w'))\n",
    "    \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Save weights if wanted\n",
    "You can reuse them by using the weights in the make_conv_set method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "all_params = [e_params, g_params, d_params]\n",
    "\n",
    "pickle.dump(all_params, open(\"model_params_420.pkl\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
